---
title: "Final Project Methods Code"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: no
    theme: cerulean
    highlight: tango
    font-family: Arial
  pdf_document:
    toc: yes
---

```{=html}
<style type="text/css">

code {
  font-family: "Consolas";
  font-size: 11px;
}

pre {
  font-family: "Consolas";
  font-size: 11px;
}

</style>
```
# Authorship

**Critical Thinking Group 1**

-   Angel Claudio
-   Bonnie Cooper
-   Manolis Manoli
-   Magnus Skonberg
-   Christian Thieme
-   Leo Yi

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=F)

set.seed(333) #ensure replicability

```

# Methodology

## Dependancies 

```{r message=F, warning= F}
library( dplyr )
library( ggplot2 )
library( gridExtra )
library( rvest )
library( tidyverse )
library( multcompView )
library( cdcfluview )
library( purrr )
library( inspectdf )
library( naniar )
library( imputeTS )
library( usmap )
library( scales )
library( ggpubr )
library(MASS)
library(caret)
library(glmnet)
library(kableExtra)
library( broom )
library( qqplotr )
```


## Data Acquisition

### PIC National Data

```{r}
# PIC National
url <- "https://raw.githubusercontent.com/SmilodonCub/DATA621/master/NCHSData12.csv"
PIC_national_df <- read.csv( url ) %>%
    rowid_to_column()
colnames( PIC_national_df ) <- c( 'rowid', 'year', 'week', 'perc_death_PI', 
                                  'perc_death_PIC', 'expected', 'threshold', 
                                  'all_deaths', 'P_deaths', 'I_deaths', 
                                  'C_deaths', 'PIC_deaths' )

# Created two new variables A.C.

PIC_national_df$perc_death_I <- # Percent of deaths from Influenza
  ( PIC_national_df$I_deaths/PIC_national_df$all_deaths)*100

PIC_national_df$perc_death_C <- # Percent of deaths from COVID-19
  ( PIC_national_df$C_deaths/PIC_national_df$all_deaths)*100

str(PIC_national_df)

```

### PIC By State Data

```{r}
# PIC State-level
url <- "https://raw.githubusercontent.com/SmilodonCub/DATA621/master/State_Custom_Data.csv"
PIC_by_state_df <- read.csv( url )
glimpse( PIC_by_state_df )
```

### Statewide Regulation Web Scrape Data

```{r}
## Statewide Restriction
url <- "http://en.wikipedia.org/wiki/U.S._state_and_local_government_responses_to_the_COVID-19_pandemic"
state_level_regulation_df <- url %>%
  read_html() %>%
  html_node(xpath='//*[@id="mw-content-text"]/div[1]/table[2]') %>%
  html_table(fill=T)
state_level_regulation_df <- state_level_regulation_df[-1,]
state_level_regulation_df <- state_level_regulation_df[,-1]
state_level_regulation_df <- state_level_regulation_df[,-11]

colnames( state_level_regulation_df ) <- c( "State_territory", "State_of_emergency_declared", "Stay_at_home_ordered",
                                            "Face_coverings_required_in_public", "Gatherings_banned", 
                                            "Out_of_state_travel_restrictions", "Closures_School", "Closures_Daycare",
                                            "Closures_Bars", "Closures_Retail")

str(state_level_regulation_df)
```

## Data Exploration

### PIC National Mortality Rate

```{r pic-national-mortality-plot}
ggplot( data = PIC_national_df, aes( x = rowid, y = perc_death_PIC, 
                                     color = '% PIC' ) ) + 
    geom_line( ) +
  
    geom_line( aes( x = rowid, y = threshold, color = 'threshold' ) ) +
  
    geom_line( aes( x = rowid, y = expected, color = 'expected' ), 
               linetype = "dashed" ) +
  
    geom_area( aes( x = rowid, y = perc_death_I, color = '% Influenza' ), 
          linetype = 'solid', fill='grey' ) +
  
    geom_area( aes( x = rowid, y = perc_death_C, color = '% COVID-19'), 
        linetype = 'solid', fill='lightblue', alpha = .2 ) +
    
    theme_classic() +
    ggtitle( 'Pneumonia, Influenza, and COVID-19 Mortality', 
             subtitle = " from the NCHS Mortality Surveillance System" ) +
    ylab( '% of All Deaths' ) +
    xlab( 'Flu Season' ) +
    scale_x_continuous(breaks=c(seq( 25,392,49)),
                       labels=c("2013-14", "2014-15", "2015-16", "2016-17", 
                                "2017-18", "2018-19", "2019-20", "2020-21")) +
  
    scale_color_manual(name="PIC data", values=c("% PIC"="red", 
                                                 # "%PI"="pink", 
                                                 'threshold'='black', 
                                                 'expected'='black',
                                                 '% Influenza'='yellow',
                                                 '% COVID-19' = 'navy'
                                                 ))
```


```{r pic-national-mortality-plot-with-pi}
ggplot( data = PIC_national_df, aes( x = rowid, y = perc_death_PIC, 
                                     color = '% PIC' ) ) + 
    geom_line( ) +
  
    geom_line( aes( x = rowid, y = threshold, color = 'threshold' ) ) +
  
    geom_line( aes( x = rowid, y = expected, color = 'expected' ), 
               linetype = "dashed" ) +
  
    geom_area( aes( x = rowid, y = perc_death_I, color = '% Influenza' ), 
          linetype = 'solid', fill='grey' ) +
  
    geom_area( aes( x = rowid, y = perc_death_PI, color = '% PI'), 
        linetype = 'solid', fill='lightblue', alpha = .2 ) +
    
    theme_classic() +
    ggtitle( 'Pneumonia, Influenza, and COVID-19 Mortality', 
             subtitle = " from the NCHS Mortality Surveillance System" ) +
    ylab( '% of All Deaths' ) +
    xlab( 'Flu Season' ) +
    scale_x_continuous(breaks=c(seq( 25,392,49)),
                       labels=c("2013-14", "2014-15", "2015-16", "2016-17", 
                                "2017-18", "2018-19", "2019-20", "2020-21")) +
  
    scale_color_manual(name="PIC data", values=c("% PIC"="red", 
                                                 # "%PI"="pink", 
                                                 'threshold'='black', 
                                                 'expected'='black',
                                                 '% Influenza'='yellow',
                                                 '% PI' = 'navy'
                                                 ))
```

```{r}
ggplot( data = PIC_national_df, aes( x = rowid, y = perc_death_PIC, color = 'PIC %' ) ) + 
    geom_line( ) +
    geom_line( aes( x = rowid, y = threshold, color = 'threshold' ) ) +
    geom_line( aes( x = rowid, y = expected, color = 'expected' ) ) +
    geom_line( aes( x = rowid, y = perc_death_PI, color = 'PI %') ) +
    theme_classic() +
    ggtitle( 'Pneumonia, Influenza, and COVID-19 Mortality', subtitle = " from the NCHS Mortality Surveillance System" ) +
    ylab( '% Deaths' ) +
    xlab( 'Flu Season' ) +
    scale_x_continuous(breaks=c(seq( 25,392,49)),
                       labels=c("2013-14", "2014-15", "2015-16", "2016-17", "2017-18", "2018-19", "2019-20", "2020-21")) +
    scale_color_manual(name="PIC data", values=c("PIC %"="red", "PI %"="blue", 'threshold'='black', 'expected'='green'))
```

### Mean Annual Influenza Death Predictions (Expected & Threshold)

```{r expected-deaths-summary}
expectedDeaths <- PIC_national_df %>%
    dplyr::select( c( 'week', 'expected', 'threshold' ) ) %>%
    dplyr::group_by(week) %>%
    dplyr::summarise( 'expected' = mean( expected ), 'threshold' = mean( threshold ) ) %>%
    dplyr::rename( WEEK = week )

head( expectedDeaths )
```

### PIC By State

```{r state-lvl-restrictions-plot}
# remove instances of this string to make later conversion easier
PIC_by_state_df[PIC_by_state_df == 'Insufficient Data'] <- NA

PIC_by_state_df <- PIC_by_state_df %>%
  # remove AREA & AGE.GROUP Uninformative, but could add later
  # if people think it'd be an interesting feature
  dplyr::select( -c( 'AREA', 'AGE.GROUP' ) ) %>% 
  mutate( NUM.INFLUENZA.DEATHS = as.numeric(NUM.INFLUENZA.DEATHS),
          NUM.PNEUMONIA.DEATHS = as.numeric(NUM.PNEUMONIA.DEATHS),
          NUM.COVID.19.DEATHS = as.numeric(NUM.COVID.19.DEATHS),
          TOTAL.PIC = as.numeric(TOTAL.PIC),
          TOTAL.DEATHS = as.numeric(TOTAL.DEATHS)) #remove commas and recast numeric

# Join the mean expected deaths by week to the df
PIC_by_state_df <- left_join( PIC_by_state_df,
                              expectedDeaths, by = "WEEK", copy = FALSE ) %>%
    mutate( PI_expected_diff = PERCENT.P.I - expected,
            PI_threshold_diff = PERCENT.P.I - threshold )

# visualize deviation from expected PI deaths for each flu season:
eachSeason_diff <- PIC_by_state_df %>%
    dplyr::rename( area = SUB.AREA, season = SEASON ) %>%
    dplyr::group_by( season, area ) %>%
    dplyr::summarise( meanPI_expected_diff = mean( PI_expected_diff, na.rm = TRUE ),
               meanPI_threshold_diff = mean( PI_threshold_diff, na.rm = TRUE )) %>%
    dplyr::mutate( season = factor( season ) )
p1 <- ggplot(data = eachSeason_diff, aes( x = season, y = meanPI_expected_diff ) ) +
    geom_boxplot() +
    geom_jitter( color="black", size = 0.4, alpha = 0.6 ) +
    theme_classic() +
    ggtitle( 'Deviance from Expected PI Mortality', 
             subtitle = 'distribution of deviation for each US state per flu season') +
    ylab( '% Deviation' ) +
    xlab( 'Flu Season' )

#ANOVA + TUKEY Test for PIC state-level
eachSeason_diff <- eachSeason_diff %>%
    mutate( season2 = gsub("-","", season ) )

#ANOVA + Tukey test
linmod <- lm( data = eachSeason_diff, meanPI_expected_diff ~ season2 )
ANOVA <- aov( linmod )
TUKEY <- TukeyHSD( x = ANOVA, 'season2', conf.level = 0.99)
TUKEY_df <- data.frame( TUKEY$season2 )
TUKEY_df <- tibble::rownames_to_column( TUKEY_df, 'Season' )
p3 <- ggplot() +
    geom_linerange( data = TUKEY_df, mapping = aes( y = Season, xmin = lwr, xmax = upr ), width = 0.2, size = 1, color = 'red' ) +
    geom_point( data = TUKEY_df, aes( x = diff, y = Season ), size = 3, shape = 21, color = 'red' ) +
    geom_vline( xintercept = 0, linetype = 'dashed', color = 'gray' ) +
    xlab( 'Difference in mean level' ) +
    ggtitle( "99% Family-wise Confidence Level" ) +
    theme_classic()
grid.arrange( p1, p3, ncol =2 )
```

### EDA on State Level Regulations

```{r eda-state-level}
glimpse( state_level_regulation_df )
state_level_regulation_df <- state_level_regulation_df %>%
  modify_if( is.character, as.factor )
p5 <- inspectdf::inspect_imb( state_level_regulation_df ) %>% show_plot()
p5 + theme_classic()
```

## Data Preparation

### Missingness in PIC by State Data

```{r}
miss_var_summary( PIC_by_state_df )
miss_var_table( PIC_by_state_df )

# Visualize the missingness by season
gg_miss_fct( x = PIC_by_state_df, fct = SEASON)
```

The figure above shows a heatmap of missingness. We see that `TOTAL.DEATHS` has a lot of missing values. that's OKay, because we will be using the `PERCENT.P.I`, `PERCENT.PIC` & features derived from the (`_diff`). However, what is troubling is that there are quite a few missing values for the 2020-21 flu season and to a lesser extend 2019-20 for what we would like to use as a predictor variable `PI_expected_diff`.

### Missingness for `PI_expect_diff` 2020-2021 by state

```{r}
states_missingness <- PIC_by_state_df %>%
  dplyr::filter( SEASON == '2020-21' ) %>%
  dplyr::select( SUB.AREA, PI_expected_diff ) %>%
  dplyr::group_by( SUB.AREA ) %>%
  miss_var_summary() %>%
  filter( n_miss != 0 ) %>%
  arrange(desc( n_miss ))

states_missingness
```

We should exclude North Carolina and consider excluding West Virginia, but otherwise perhaps find a clever way to impute the other missing values.
First I will visualize the impact of the missingness on the target variable, `PI_expected_diff`

Let's see if there is a pattern for missingness by WEEK:
```{r}
week_missingness <- PIC_by_state_df %>%
  dplyr::filter( SEASON == '2020-21', !SUB.AREA %in% c( 'North Carolina', 'West Virginia' ) ) %>%
  dplyr::select( WEEK, PI_expected_diff ) %>%
  dplyr::group_by( WEEK ) %>%
  miss_var_summary() %>%
  filter( n_miss != 0 ) %>%
  arrange(desc( n_miss ))

week_missingness
```
After filtering West Virginia and North Carolina, we can see that 22% of the remaining states are missing data for week 9. Week 9 is the last week of the 2020-2021 flu season to be included in the data set, so the missingness is probably due to lags in reporting. We will drop week 9 from further analysis and impute the remainder of values using kalman smoothing methods from the `imputeTS` library.

```{r}
PI_missingness_2 <- PIC_by_state_df %>%
  dplyr::filter( SEASON == '2020-21') %>%
  dplyr::select( SUB.AREA, WEEK, PI_expected_diff ) %>%
  dplyr::group_by( SUB.AREA ) %>%
  dplyr::mutate( PI_expected_diff_imp = na_kalman( PI_expected_diff ),
          num = row_number() )
PI_missingness_2  
```

Visualize the imputation the make sure it behaves:
```{r}
ggplot( data = PI_missingness_2, aes( x = num, y = PI_expected_diff_imp ) ) +
  geom_line( color = 'red' ) +
  geom_line( data = PI_missingness_2, aes( x = num, y = PI_expected_diff ), color = 'blue' ) +
  facet_wrap( ~ SUB.AREA )
```
We don't observe any obvious disagreement with the imputation results (red). This plot is also important because it reinforces our earlier decision to remove North Carolina and West Virginia due to missingness.

Moving forward we will perform the following to prepare the data:  

* remove North Carolina & West Virginia
* restrict the WEEKs to those included in the data for the 2020-2021 season - week 9.
* combine the data for New York and New York City

```{r}
PIC_by_state_imp <- PIC_by_state_df %>%
  dplyr::filter( !SUB.AREA %in% c( 'North Carolina', 'West Virginia' ) ) %>% #remove the states that are missing a lot of 2020-21 data
                 #filter only the WEEKs that have adequate data for 2020-21
                 #WEEK %in% c( 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 1, 2, 3, 4, 5, 6, 7, 8 ) ) %>%
  dplyr::select( -c(TOTAL.DEATHS, PERCENT.COMPLETE) ) #remove the total deaths columns due to missingness
```

```{r}
#combine the New York Data
PIC_NY <- PIC_by_state_imp %>%
  dplyr::filter( SUB.AREA %in% c( 'New York', 'New York City' ) ) %>%
  mutate( est_Total_Deaths = round(((NUM.INFLUENZA.DEATHS + NUM.PNEUMONIA.DEATHS + NUM.COVID.19.DEATHS) * 100)/PERCENT.PIC, 0 ) ) %>%
  dplyr::group_by( SEASON, WEEK ) %>%
  dplyr::summarise( NUM.INFLUENZA.DEATHS = sum( NUM.INFLUENZA.DEATHS ), 
             NUM.PNEUMONIA.DEATHS = sum( NUM.PNEUMONIA.DEATHS ), 
             NUM.COVID.19.DEATHS = sum( NUM.COVID.19.DEATHS ),
             TOTAL.PIC = sum( TOTAL.PIC ),
             est_Total_Deaths = sum( est_Total_Deaths ),
             expected = expected,
             threshold = threshold) %>%
  mutate( PERCENT.P.I = 100*(NUM.INFLUENZA.DEATHS + NUM.PNEUMONIA.DEATHS)/est_Total_Deaths,
          PERCENT.PIC = 100*(TOTAL.PIC)/est_Total_Deaths,
          PI_expected_diff = PERCENT.P.I - expected,
          PI_threshold_diff = PERCENT.P.I - threshold,
          SUB.AREA = 'New York' ) %>%
  distinct() %>%
  dplyr::select( -est_Total_Deaths )
```

Concatenate the combined New York + New York City data with the data from the rest of the state (minus W Virginia & N Carolina)
```{r}
#sort( colnames( PIC_by_state_imp ) ) == sort( colnames( PIC_NY ) )

PIC_notNY <- PIC_by_state_imp %>%
  dplyr::filter( !SUB.AREA %in% c( 'New York', 'New York City' ) )

PIC_by_state_clean <- rbind( PIC_NY, PIC_notNY )
```


Visualization of States map:
```{r}
diff_by_state_plot <- PIC_by_state_clean %>%
  filter( SEASON == '2020-21' ) %>%
  group_by( SUB.AREA) %>%
  summarise( max_PI_expected_diff = max( PI_expected_diff, na.rm = T ) ) %>%
  mutate( fips = fips( SUB.AREA ) )
pdiff <- plot_usmap( data = diff_by_state_plot, values = 'max_PI_expected_diff', labels = F ) +
  scale_fill_continuous( low = 'white', high = 'red',
                         name = 'Diff in PI', label = scales::comma,
                         limits = c( 0,25 ) ) +
  theme( legend.position = 'right' ) +
  theme( panel.background = element_rect( colour = "red"))

PIC_by_state_plot <- PIC_by_state_clean %>%
  filter( SEASON == '2020-21' ) %>%
  group_by( SUB.AREA) %>%
  summarise( max_percent_PIC = max( PERCENT.PIC, na.rm = T ) ) %>%
  mutate( fips = fips( SUB.AREA ) )
pperPIC <- plot_usmap( data = PIC_by_state_plot, values = 'max_percent_PIC', labels = F ) +
  scale_fill_continuous( low = 'white', high = 'red',
                         name = 'Percent PIC', label = scales::comma,
                         limits = c( 0,60 ) ) +
  theme( legend.position = 'right' ) +
  theme( panel.background = element_rect( colour = "red"))

grid.arrange( pdiff, pperPIC, ncol = 2 )
```

Visualize the relationship between excess PI mortality and PIC mortality
```{r}

relation_plot <- PIC_by_state_plot
relation_plot$max_PI_expected_diff <- diff_by_state_plot$max_PI_expected_diff
relation_plot <- relation_plot %>%
  dplyr::arrange( max_percent_PIC ) 

#this dataset has been uploaded to B Cooper's git for plotting in the manuscript
# https://raw.githubusercontent.com/SmilodonCub/DATA621/master/PIC_PIexcess_relation.csv
#write.csv( relation_plot, 'PIC_PIexcess_relation.csv' )
prel <- ggplot( data = relation_plot, aes( x = max_percent_PIC, y = max_PI_expected_diff, label = SUB.AREA ) ) +
  geom_point() +
  geom_smooth( method = 'lm', se = F, color = 'black' ) +
  stat_cor( r.digits = 3, p.accuracy = 0.001 ) +
  geom_label(check_overlap = TRUE, nudge_y = 0.75) +
  theme_classic() +
  ggtitle('Excess PI mortality vs PIC Mortality for the 2020-2021 flu season') +
  ylab( 'Max. Weekly Excess PI Mortality' ) +
  xlab( 'Max. Weekly PIC Mortality' )
prel
```

We see a linear relationship between the excess PI mortality and PIC mortality.
The goal of our modeling and further analysis will be to see if variables that describe state-level COVID-19 regulatory measure and/or demographic features (e.g. state population ) can add to the explanatory power of this relationship.



Now we prepare to merge the state-level PIC data with the state-level regulation data
Checking Unique States in Each Source

```{r}
# check to see how joining on state looks
d_pbs2 <- data.frame( SUB.AREA = unique( PIC_by_state_clean$SUB.AREA ) )
d_pbs2$state = d_pbs2$SUB.AREA

d_pbs <- distinct(PIC_by_state_df, SUB.AREA)
d_pbs$state = d_pbs$SUB.AREA

d_reg <- distinct(state_level_regulation_df, State_territory)
d_reg$state <- d_reg$State_territory

key_check <- full_join(d_pbs2, d_reg, by = c('state' = 'state')) %>%
  filter(is.na(SUB.AREA) | is.na(State_territory))
key_check
```

This output shows the mismatches in state names between the PIC and COVID regulation state-level data. We will change the regulation `California (government response)` to `California` and drop all territories from the regulation data that are not present in the state-level PIC.

```{r}
state_level_regulation_df$State_territory <- as.character( state_level_regulation_df$State_territory )
state_level_regulation_df$State_territory[ state_level_regulation_df$State_territory == 'California (government response)' ] <- 'California'
state_level_regulation_df$State_territory <- factor( state_level_regulation_df$State_territory )
state_level_regulation_df$State_territory
```

```{r}
df <- inner_join(data.frame(PIC_by_state_clean), state_level_regulation_df, by = c('SUB.AREA' = 'State_territory'))
names(df) <- lapply(names(df), tolower)
str(df)
```

Some visual explorations of the differences in observed and expected PI mortality in relation to state-level regulations
```{r}
# state of emergency
df$soe_week <- as.Date(paste0(df$state_of_emergency_declared, ' 2020'), '%B %d %Y') %>%
  strftime(format = '%V') %>%
  as.numeric()

# stay at home
df$sah_week <- as.Date(paste0(df$stay_at_home_ordered, ' 2020'), '%B %d %Y') %>%
  strftime(format = '%V') %>%
  as.numeric()

# sample new york view
ny <- filter(df, sub.area == 'New York' & season %in% c('2019-20', '2020-21'))

ggplot(ny, aes(x = week, y = pi_expected_diff, color = season)) +
  geom_point() +
  geom_vline(xintercept = ny$soe_week, color = 'blue') +
  geom_vline(xintercept = ny$sah_week, color = 'purple')

```

This visualization depicts the weekly excess PI mortality rates for the 2020-2021 (blue) and 2019-2020 (red) flu seasons with vertical lines to show when the New York State government restrictions for the declaration of a state of emergency (blue) and the stay at home order (purple) were enacted.


```{r}
df2 <- filter(df, season %in% c('2019-20', '2020-21'))

# add weeks after 52
df2$week_v2 <- with(df2, ifelse(season == '2020-21' & week >= 40, (week - 40 + 1) + 52, ifelse(season == '2020-21' & week < 40, week + 52 + 14, week)))

# check against ny, with references lines for state of emergency and stay at home orders
pNY <- filter(df2, sub.area == 'New York') %>%
  ggplot(aes(x = week_v2, y = pi_expected_diff, color = season)) +
  geom_point() +
  geom_vline(aes(xintercept = soe_week), color = 'blue') +
  geom_vline(aes(xintercept = sah_week), color = 'purple') +
  ggtitle( 'New York State' )

# facet by state
pStates <- ggplot(df2, aes(x = week_v2, y = pi_expected_diff, color = season)) +
  geom_point() +
  geom_vline(aes(xintercept = soe_week), color = 'blue') +
  geom_vline(aes(xintercept = sah_week), color = 'purple') +
  facet_wrap(~sub.area, scales = 'free_y') +
  ggtitle( 'U.S. States' )

#distinct(df2, sub.area, soe_week, sah_week, face_coverings_required_in_public)
#grid.arrange( pNY, pStates, ncol = 2, widths = c( 6,10 ) )
pNY
pStates
```

These visualization plot the weekly data contiguously. There is a detailed plot for New York as well as an overview plot for all states.

Here we incorporate state-level population data from the U.S. census and look at some other visualizations of our target variable for excess PI mortality `PI_Expected_diff` in relation to the state-level COVID-19 restrictions. These visualization are informative on the subsequent data processing steps to ready the data set for modelling as comments adjacent to the plots code explain.
```{r}
url <- 'https://raw.githubusercontent.com/SmilodonCub/DATA621/master/nst-est2019-alldata.csv'
pop <- read.csv(url)

names(pop) <- lapply(names(pop), tolower)
#str(pop)
#table(pop$name)

pop_est <- dplyr::select(pop, name, popestimate2019)

#MS: added population density as a feature
#url2 <- 'https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-areas.csv'
#area <- read.csv(url2)

df2 <- left_join(df2, pop_est, by = c('sub.area' = 'name'))
#df2 <- left_join(df2, area, by = c('sub.area' = 'state'))

str(df2)

df2$pct_covid_deaths <- df2$num.covid.19.deaths / df2$popestimate2019 
#df2$pop_density <- df2$popestimate2019 / df2$area..sq..mi.
#df2$pop_density #verify

#pop_den <- select(df2, sub.area, pop_density)

```


```{r}
## Percent of population dying from covid
ggplot(df2, aes(x = week_v2, y = pct_covid_deaths)) +
  geom_line() +
  facet_wrap(~sub.area)

## percent pi vs pic
ggplot(df2, aes(x = week_v2, y = percent.p.i / 100)) +
  geom_line() +
  geom_line(aes(y = percent.pic / 100), color = 'orange') +
  facet_wrap(~sub.area)
  

## showing different facets
ggplot(df2, aes(x = week_v2, y = pi_expected_diff, color = sub.area)) +
  geom_line() +
  facet_wrap(~face_coverings_required_in_public, nrow = n_distinct(df$face_coverings_required_in_public))

### maybe look at averages instead
## face coverings v2
group_by(df2, week_v2, face_coverings_required_in_public) %>%
  summarize(avg_excess_pi = mean(pi_expected_diff, na.rm = T),
            .groups = 'drop') %>%
  ggplot(aes(x = week_v2, y = avg_excess_pi, color = face_coverings_required_in_public)) +
  geom_line() +
  theme(legend.position = 'bottom')


## gatherings banned
group_by(df2, week_v2, gatherings_banned) %>%
  summarize(avg_excess_pi = mean(pi_expected_diff, na.rm = T),
            .groups = 'drop') %>%
  ggplot(aes(x = week_v2, y = avg_excess_pi, color = gatherings_banned)) +
  geom_line()


## travel restrictions
group_by(df2, week_v2, out_of_state_travel_restrictions) %>%
  summarize(avg_excess_pi = mean(pi_expected_diff, na.rm = T),
            .groups = 'drop') %>%
  ggplot(aes(x = week_v2, y = avg_excess_pi, color = out_of_state_travel_restrictions)) +
  geom_line()


## closures - school
group_by(df2, week_v2, closures_school) %>%
  summarize(avg_excess_pi = mean(pi_expected_diff, na.rm = T),
            .groups = 'drop') %>%
  ggplot(aes(x = week_v2, y = avg_excess_pi, color = closures_school)) +
  geom_line()

## closures - daycare
group_by(df2, week_v2, closures_daycare) %>%
  summarize(avg_excess_pi = mean(pi_expected_diff, na.rm = T),
            .groups = 'drop') %>%
  ggplot(aes(x = week_v2, y = avg_excess_pi, color = closures_daycare)) +
  geom_line()

## closures - bars
group_by(df2, week_v2, closures_bars) %>%
  summarize(avg_excess_pi = mean(pi_expected_diff, na.rm = T),
            .groups = 'drop') %>%
  ggplot(aes(x = week_v2, y = avg_excess_pi, color = closures_bars)) +
  geom_line()

## closures - retail
group_by(df2, week_v2, closures_retail) %>%
  summarize(avg_excess_pi = mean(pi_expected_diff, na.rm = T),
            .groups = 'drop') %>%
  ggplot(aes(x = week_v2, y = avg_excess_pi, color = closures_retail)) +
  geom_line()

## state of emergency timing
group_by(df2, week_v2, soe_week) %>%
  summarize(avg_excess_pi = mean(pi_expected_diff, na.rm = T),
            .groups = 'drop') %>%
  ggplot(aes(x = week_v2, y = avg_excess_pi, color = factor(soe_week))) +
  geom_line()


## stay at home timing
group_by(df2, week_v2, sah_week) %>%
  summarize(avg_excess_pi = mean(pi_expected_diff, na.rm = T),
            .groups = 'drop') %>%
  ggplot(aes(x = week_v2, y = avg_excess_pi, color = factor(sah_week))) +
  geom_line()

```

```{r}
model_dat <- df %>%
  filter( week %in% c( 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 1, 2, 3, 4, 5, 6, 7, 8 )) %>%
  dplyr::group_by( sub.area ) %>%
  dplyr::mutate( PI_expected_diff_imp = na_kalman( pi_expected_diff ),
                 num = row_number() )




#get population density
pop_density <- read.csv('https://www2.census.gov/programs-surveys/decennial/2020/data/apportionment/apportionment.csv')
pop_density_2020 <- pop_density %>%
  dplyr::filter(Year == 2020) %>%
  dplyr::select(Name, pop_density = Resident.Population.Density) %>%
  dplyr::mutate(pop_density = as.numeric(str_replace(pop_density,',','')))


#join population data
model_dat <- left_join(model_dat, pop_est, by = c('sub.area' = 'name')) %>%
  left_join(pop_density_2020, by = c('sub.area' = 'Name'))


# engineer a feature from previous non-COVID-19 years (before 2019-2020) flu data to take the mean difference between observed and expected PI mortality
preCOVID_diff <- model_dat %>%
  filter( !season %in% c('2019-20', '2020-21') ) %>%
  rowwise() %>%
  mutate( season2 = list(strsplit(season, split = "-")[[1]][1])) %>%
  dplyr::select( c( season2, week, sub.area, pi_expected_diff )) %>%
  pivot_wider( names_from = season2, names_prefix = 'S', values_from = pi_expected_diff) %>%
  mutate( mean_preCOVID_diff = rowMeans(dplyr::select( ., S2013:S2018), na.rm = TRUE) ) %>%
  dplyr::select( c( week, sub.area, mean_preCOVID_diff ) )

#join population density
#model_dat <- left_join(model_dat, pop_den, by = c('sub.area' = 'sub.area')) #MS: commented out because it reduced the (model 3) R-squared from 0.84 to 0.8371

train_dat <- model_dat %>%
  filter( season == '2020-21' )

#merge the preCOVID_diff feature
train_dat <- merge( train_dat, preCOVID_diff, by.x = c('week', 'sub.area'), by.y = c('week', 'sub.area' ))

#remove features that won't contribute to the model
train_dat <- train_dat %>%
  dplyr::select( -c( season, num.influenza.deaths, num.pneumonia.deaths, num.covid.19.deaths, closures_school,
              total.pic, state_of_emergency_declared, stay_at_home_ordered, pi_threshold_diff, threshold, ) ) %>%
  mutate_if( is.character, as.factor )
glimpse( train_dat )
```
```{r}
summary( train_dat )
```

98% of states closed schools for the remainder of the term, so we will not consider this feature. Here we create flags for several of the regulations variables to simplify the modeling approach:
```{r}

train_dat <- train_dat %>%
  mutate( yes_closures_bars = closures_bars == 'Yes', # Bar closures == Yes
          yes_closures_daycare = closures_daycare == 'Yes', # Daycare closures == Yes
          yes_closures_retail = closures_retail == 'Yes', # retail closures == Yes
          yes_face_coverings = face_coverings_required_in_public == 'Yes', # Face coverings required in public == Yes
          yes_travel_restrictions = out_of_state_travel_restrictions != 'No', 
          # Out of State travel restrictions != No (some form or restriction enacted)
          all_gatherings_banned = gatherings_banned == 'All' ) %>% # All Gathering Banned 
    
  mutate_if( is.logical, as.factor )
```

```{r}
str(train_dat)
```


## Model Building

train + validation set
```{r}
#Train-test split the sample
splitSample <- sample(1:2, size=nrow(train_dat), prob=c(0.75,0.25), replace = TRUE)
train_PI <- train_dat[splitSample==1,]
test_PI <- train_dat[splitSample==2,]

summary(train_PI$yes_closures_bars)
summary(test_PI$yes_closures_bars)

#Handle NA values
train_PI <- na.omit(train_PI)
test_PI <- na.omit(test_PI)

summary(train_PI$yes_closures_bars)
summary(test_PI$yes_closures_bars)
```



We will be using the linear model of Excess PI Mortality ~ PIC Mortality as a benchmark  

### Model 1 Simple Linear Regression

```{r}
model1 <- lm( data = train_PI, PI_expected_diff_imp ~ percent.pic )
summary( model1 )
```

```{r}
ggplot( data = train_PI, aes( x = percent.pic, y = PI_expected_diff_imp ) ) +
  geom_point() +
  geom_smooth( method = 'lm', se = F, color = 'black' ) +
  theme_classic()
```

```{r}

#Evaluation metrics for Linear Models:
eval_metrics = function(model, df, predictions, target){
    resids = df[,target] - predictions
    resids2 = resids**2
    N = length(predictions)
    r2 = as.character(round(summary(model)$r.squared, 4))
    adj_r2 = as.character(round(summary(model)$adj.r.squared, 4))
    print(adj_r2) #Adjusted R-squared
    print(as.character(round(sqrt(sum(resids2)/N), 4))) #RMSE
}

# Predict and evaluate model1 on training data
predictions = predict(model1, newdata = train_PI)
eval_metrics(model1, train_PI, predictions, target = 'PI_expected_diff_imp') 

# Predict and evaluate model1 on test data
predictions = predict(model1, newdata = test_PI)
eval_metrics(model1, test_PI, predictions, target = 'PI_expected_diff_imp')
```


Diagnostic plots

```{r}
plot( model1 )
```

The simple linear model gives a statistically significant fit the the data and has a relatively high $R^2$ value. However, there are some indications from the diagnostic plots that the variance of residuals is not constant across the data series. We will attempt to explain more of the variance by adding features to the model that will hopefully increase the predictive power.  

### Model2 Multiple Linear Regression  

Adding the engineered binary feature variables for government regulations:  

* **soe_week**
* **sah_week**
* **yes_closure_bars**
* **yes_closures_daycare**
* **yes_closures_retail**
* **yes_face_coverings**
* **yes_travel_restrictions**


```{r}
mlm <- PI_expected_diff_imp ~ percent.pic + soe_week + sah_week  + yes_closures_daycare + yes_closures_retail +
  yes_face_coverings + yes_travel_restrictions + all_gatherings_banned 

model2 <- lm( formula = mlm, data = train_PI )
summary( model2 )
```

AIC step-wise feature selection for model 2:
```{r}
step_m2 <- step( model2 )
summary( step_m2 )

# Predict and evaluate model2 on training data
predictions = predict(step_m2, newdata = train_PI)
eval_metrics(step_m2, train_PI, predictions, target = 'PI_expected_diff_imp')

# Predict and evaluate model2 on test data
predictions = predict(step_m2, newdata = test_PI)
eval_metrics(step_m2, test_PI, predictions, target = 'PI_expected_diff_imp')
```

```{r}
mlm <- PI_expected_diff_imp ~ percent.pic + soe_week + yes_closures_retail +  
  yes_face_coverings + pop_density + popestimate2019

model2b <- lm( formula = mlm, data = train_PI )
summary( model2b )
```


### Model 3 AIC 

we now build a similar multiple regression model as in Model 2, However, we use the multi-level categorical features. This tests the explanatory power of the features as the data was originally scraper. Model 2 uses the simplified binary flag engineered features.
Adding the following feature variables:  

* **mean_preCOVID_diff** - what is the weekly mean difference in observed and predicted PI mortality for each state. 
* **percent.pic** - weekly percent of deaths due to PI
* **popestimate2019**
* **pop_density**
* **soe_week**
* **sah_week**
* **closure_bars**
* **closures_daycare**
* **closures_retail**
* **face_coverings**
* **travel_restrictions**
* **travel_restrictions**



```{r model-3}
mlm3 <- PI_expected_diff_imp ~ mean_preCOVID_diff + percent.pic + popestimate2019 + #pop_density +
  soe_week + sah_week  + closures_daycare + closures_retail +
  face_coverings_required_in_public + out_of_state_travel_restrictions + pop_density

model3 <- lm( formula = mlm3, data = train_PI )
summary( model3)

```


```{r}
# added below to actual transformation of model3 using step and assigning it
# to variable step_m3 to represent model 3 - A.C.

step_m3 <- step(model3)
summary(step_m3)

# Predict and evaluate model3 on training data
predictions = predict(step_m3, newdata = train_PI)
eval_metrics(step_m3, train_PI, predictions, target = 'PI_expected_diff_imp')

# Predict and evaluate model3 on test data
predictions = predict(step_m3, newdata = test_PI)
eval_metrics(step_m3, test_PI, predictions, target = 'PI_expected_diff_imp')
```

```{r}
step_m3$call
```
```{r}
coefs <- tidy( step_m3 )
ordered_M3coefs <- coefs[order(coefs$estimate, decreasing = TRUE),]
```


### Model 3.5: Multivariate Model with AIC


```{r model-3.5, eval=T}
cols_remove <- c("closures_bars",    # constant error (only 1 lvl w/values)
                 "yes_closures_bars",# constant error (only 1 lvl w/values)
                 "mean_preCOVID_diff",
                 "pi_expected_diff",
                 "expected",
                 "week", 
                 "soe_week",
                 "sah_week",
                 "num",
                 "percent.p.i",     # huge decline in prediction power
                 "percent.pic"      # huge decline in prediction power
                 )

train_PI_excluding_constant_categories <- 
  train_PI[, !(colnames(train_PI) %in% cols_remove)]


mlm3.5 <- lm(PI_expected_diff_imp ~ .,
             data=train_PI_excluding_constant_categories)

summary(mlm3.5)

step_mlm3.5 <- step(mlm3.5)
summary(step_mlm3.5)

# Predict and evaluate model3.5 on training data
predictions = predict(step_mlm3.5, newdata = train_PI)
eval_metrics(step_mlm3.5, train_PI, predictions, target = 'PI_expected_diff_imp')

# Predict and evaluate model3.5 on test data
predictions = predict(step_mlm3.5, newdata = test_PI)
eval_metrics(step_mlm3.5, test_PI, predictions, target = 'PI_expected_diff_imp')
```

    
### Model 4: Ridge Regression Model

Ridge regression is an extension of linear regression where model complexity and the potential for over-fitting are addressed by adding a penalty parameter to penalize large coefficients.

One of the major differences between linear and regularized regression is the use of a lambda tuning parameter. To automate the task of finding the optimal lambda value, we made use of the cv.glmnet() function.

*Note: in order to run this model we had to omit NAs*
    
```{r}
#MS added: pop density as feature (above) to no avail, regularization (to avoid overfitting) below

#Ridge Regression Model

#Specify column names
cr = c('percent.pic', 'popestimate2019', 'sah_week', 'closures_daycare', 'closures_retail', 'out_of_state_travel_restrictions', 'PI_expected_diff_imp')

#head(train_PI)

#Generate dummy variables from data (if applicable)
dummies <- dummyVars(PI_expected_diff_imp ~ percent.pic + popestimate2019 + 
    sah_week + closures_daycare + closures_retail + out_of_state_travel_restrictions, data = train_PI[,cr])
train_dummies = predict(dummies, newdata = train_PI[,cr]) #727 x 18
test_dummies = predict(dummies, newdata = test_PI[,cr]) #180 x 18
#print(dim(train_dummies)); print(dim(test_dummies))

#train_PI <- train_dat[splitSample==1,]
#valid_PI <- train_dat[splitSample==2,]
#test_PI <- train_dat[splitSample==3,]

#Create numeric model matrices
x = as.matrix(train_dummies)
y_train = train_PI$PI_expected_diff_imp
x_test = as.matrix(test_dummies)
y_test = test_PI$PI_expected_diff_imp

lambdas <- 10^seq(2, -3, by = -.1) #specify lambda sequence

###LEFT OFF HERE: ridge + LASSO regression ...

#Train model
ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 0, family = 'gaussian', lambda = lambdas, winsfrac=0)
summary(ridge_reg)

#Compute optimal lambda
cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
ol <- cv_ridge$lambda.min
ol #0.02511886

#Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- round((1 - SSE / SST),4)
  RMSE = round(sqrt(SSE/nrow(df)),4)
  
  # Model performance metrics
data.frame(RMSE = RMSE, Rsquare = R_square)
}

#Predict and evaluate raw_ridge_reg model on training data
predictions_train <- predict(ridge_reg, s = ol, newx = x)
eval_results(y_train, predictions_train, train_PI)

#Predict and evaluate raw_ridge_reg model on testing data
predictions_test <- predict(ridge_reg, s = ol, newx = x_test)
eval_results(y_test, predictions_test, test_PI)

#Ridge regression lambda plot
plot(ridge_reg)

```

When lambda is zero, the penalty term has no effect, whereas when lambda increases to infinity, the shrinkage penalty grows and our coefficients approach zero. Our optimal lambda was computed as 0.02511886 and we can see from the above plot the critical role optimal lambda computation can have when evaluating our model.

We used the glmnet package to build our regularized regression models (with alpha = 0 for ridge regression and alpha = 1 for lasso regression). Being that the corresponding function does not work with dataframes, we used the dummyVars() function from the caret package to create our model matrices and then used the predict() function to create numeric model matrices for both training and test data.


### Model 5: Lasso Regression Model

LASSO (Least Absolute Shrinkage and Selection Operator) regression is an extension of linear regression where model complexity and the potential for over-fitting are addressed by limiting the sum of the absolute values of model coefficients.

Similar to ridge regression, the first step was to find the optimal lambda value and we automated this task through the use of the cv.glmnet() function.

```{r}

#Lasso Regression Model

# Setting alpha = 1 implements lasso regression
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)

#Compute optimal lambda
lambda_best <- lasso_reg$lambda.min 
lambda_best #0.01995262

lasso <- glmnet(x, y_train, alpha = 1, lambda = lambda_best, standardize = TRUE)

predictions_train <- predict(lasso, s = lambda_best, newx = x)
eval_results(y_train, predictions_train, train_PI)

predictions_test <- predict(lasso, s = lambda_best, newx = x_test)
eval_results(y_test, predictions_test, test_PI)

#Lasso regression lambda plots
op <- par(mfrow=c(1, 2))
plot(lasso_reg$glmnet.fit, "norm",   label=TRUE)
plot(lasso_reg$glmnet.fit, "lambda", label=TRUE)
par(op)

```

When lambda approaches zero, the loss function of our model approaches the OLS function and we consider more variables, whereas when lambda grows, the regularization term has a greater effect and we see fewer variables in the model. Our optimal lambda was computed as 0.01995262. We observed in the plots above the effect our lambda value (on different scales) had on corresponding coefficient values.

A similar model-fitting approach to ridge regression was taken (utilizing the glmnet package).

## Model Selection

We output the performance metrics of all models in a table to select the model with the strongest predictive promise. We evaluated performance using the R-squared value and Root Mean Squared Error (RMSE). Lower RMSE and higher R-squared values were indicative of a stronger model.

To succinctly summarize our model optimization attempts and results, we captured the method used as well as the number of variables considered and corresponding evaluation metrics for training and test data:

```{r}

#Create Kable table to succinctly summarize model optimization results
Model <- c('1', '2', '3', '4', '5')
Method <- c('Linear', 'Linear', 'Linear', 'Ridge', 'Lasso')
Var_Num <- c(1, 7, 8, 6, 6)
R2_train <- c(0.7934, 0.8101, 0.8326, 0.8297, 0.8296)
RMSE_train <- c(2.3605, 2.254, 2.1029, 2.1446, 2.1457)
R2_test <- c(0.7934, 0.8101, 0.8267, 0.8257, 0.8258) #this is where Regularization methods should shine ...
RMSE_test <- c(2.4184, 2.2984, 2.2058, 2.2393, 2.2388)

output <- cbind(Model, Method, Var_Num, R2_train, RMSE_train, R2_test, RMSE_test)

output %>%
  kbl(caption = "Regression Model Comparison") %>%
  kable_minimal() %>%
  kable_styling(latex_options = "hold_position")
```

From the above model statistics, we can extend that:

* **Var_Num**: while all models consider a relatively low number of variables, `Model 1`, the simple linear model, is clearly the simplest.
* **R2_train**: Model 4 is the highest performing, followed by Models 5 and 3. A higher R squared value for this column is indicative of stronger predictive performance for (seen) training data. Regularization and raw (broader) data appears to be correlated with stronger predictive outcomes on training data.
* **RMSE_train**: Models 1, 5, and then 4 have the lowest error for (seen) training data,
* **R2_test**: Model 3 is the highest performing (by a relatively significant margin), followed by Models 2, 5 and then 4. A higher R squared value for this column is indicative of stronger predictive performance on (unseen) testing data. 
* **RMSE_test**: Models 5, 4, and then 3 have the lowest error for (unseen) testing data. Regularization methods appear to reduce error.

To find the point of balance between model complexity, goodness of fit and error, we elect Model 3. The inclusion of population data in conjunction with the elimination of impertinent variables via step() function appears to have led to our strongest performing model. Model 3 was the most promising of models due to its simplicity, relatively low error rate (all models performed well here), and strong performance on unseen data. This last metric, unseen data performance, was the deciding factor. 

Model 3 Evaluation:

```{r}
#png(file="/home/bonzilla/Documents/MSDS/critical-group-1/Final_Project/final_project_report/saving_mod3diagnostics.png",width=800, height=350)
step_m3_aug <- augment( step_m3 )
ResFitsp <- ggplot( data = step_m3_aug, aes( x = .fitted, y = .resid ) ) +
    geom_point(fill = NA, shape = 21, alpha = 0.5, size = 2 ) +
    ggtitle( "Residuals versus Fits plot" ) +
    geom_hline( yintercept = 0, color = 'black' ) +
    ylab( "Residual" ) +
    xlab( "Fitted Value" ) +
    theme_classic()
NormResp <- ggplot( data = step_m3_aug, mapping =  aes( sample = .resid ) ) +
    stat_qq_point( fill = NA, shape = 21, alpha = 0.5, size = 2 ) +
    stat_qq_line( color = 'black' ) +
    ggtitle( "Normal plot of residuals" ) +
    xlab( "Residual" ) +
    ylab( "Percent" ) +
    theme_classic()
grid.arrange( ResFitsp, NormResp, ncol = 2 )
#dev.off()
```
Incorporating demographic and government restriction variables as in Model 3 
```{r}
# model 1 & model 3 are nested models, so the Chi^2 test makes sense here.
# this anova comparison tests whether the change in residual sum of squares is statistically significant or not
anova( model1, step_m3, test = "Chisq" )
```




<br><br><br>