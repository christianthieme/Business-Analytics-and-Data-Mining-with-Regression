# Libraries and Options
knitr::opts_chunk$set(echo = F, warning = F, message = F, eval = T,
fig.height = 5, fig.width = 10)
library(knitr)
library(skimr)
library(visdat)
library(inspectdf)
library(corrplot)
library(scales)
library(tidyverse)
library(tidyr)
library(bestglm)
library(pROC)
library(car)
library(ggcorrplot)
library(mice)
library(caret)
library(plyr)
library(dplyr)
library(MASS)
library(zoo)
options(scipen = 9)
set.seed(123)
boxplot_depend_vs_independ <- function(df_train, target_name) {
train_int_names <- df_train %>% select_if(is.numeric)
int_names <- names(train_int_names)
myGlist <- vector('list', length(int_names))
names(myGlist) <- int_names
for (i in int_names) {
myGlist[[i]] <-
ggplot(df_train, aes_string(x = target_name, y = i)) +
geom_boxplot(color = 'steelblue', outlier.color = 'firebrick',
outlier.alpha = 0.35) +
labs(title = paste0(i,' vs target'), y = i, x= 'target') +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.45),
panel.grid.major.y =  element_line(color = "grey",
linetype = "dashed"),
panel.grid.major.x = element_blank(),
panel.grid.minor.y = element_blank(),
panel.grid.minor.x = element_blank(),
axis.ticks.x = element_line(color = "grey")
)
}
myGlist <- within(myGlist, rm(target_name))
gridExtra::grid.arrange(grobs = myGlist, ncol = 3)
}
plot_corr_matrix <- function(dataframe, significance_threshold){
title <- paste0('Correlation Matrix for significance > ',
significance_threshold)
df_cor <- dataframe %>% mutate_if(is.character, as.factor)
df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
#run a correlation and drop the insignificant ones
corr <- cor(df_cor)
#prepare to drop duplicates and correlations of 1
corr[lower.tri(corr,diag=TRUE)] <- NA
#drop perfect correlations
corr[corr == 1] <- NA
#turn into a 3-column table
corr <- as.data.frame(as.table(corr))
#remove the NA values from above
corr <- na.omit(corr)
#select significant values
corr <- subset(corr, abs(Freq) > significance_threshold)
#sort by highest correlation
corr <- corr[order(-abs(corr$Freq)),]
#print table
# print(corr)
#turn corr back into matrix in order to plot with corrplot
mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
#plot correlations visually
corrplot(mtx_corr,
title=title,
mar=c(0,0,1,0),
method='color',
tl.col="black",
na.label= " ",
addCoef.col = 'black',
number.cex = .9)
}
VIM::aggr(final_df, col=c('green','red'), numbers=T, sortVars=T,
cex.axis = .7,
ylab=c("Proportion of Data", "Combinations and Percentiles"))
# Libraries and Options
knitr::opts_chunk$set(echo = F, warning = F, message = F, eval = T,
fig.height = 5, fig.width = 10)
library(knitr)
library(skimr)
library(visdat)
library(inspectdf)
library(corrplot)
library(scales)
library(tidyverse)
library(tidyr)
library(bestglm)
library(pROC)
library(car)
library(ggcorrplot)
library(mice)
library(caret)
library(plyr)
library(dplyr)
library(MASS)
library(zoo)
options(scipen = 9)
set.seed(123)
boxplot_depend_vs_independ <- function(df_train, target_name) {
train_int_names <- df_train %>% select_if(is.numeric)
int_names <- names(train_int_names)
myGlist <- vector('list', length(int_names))
names(myGlist) <- int_names
for (i in int_names) {
myGlist[[i]] <-
ggplot(df_train, aes_string(x = target_name, y = i)) +
geom_boxplot(color = 'steelblue', outlier.color = 'firebrick',
outlier.alpha = 0.35) +
labs(title = paste0(i,' vs target'), y = i, x= 'target') +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.45),
panel.grid.major.y =  element_line(color = "grey",
linetype = "dashed"),
panel.grid.major.x = element_blank(),
panel.grid.minor.y = element_blank(),
panel.grid.minor.x = element_blank(),
axis.ticks.x = element_line(color = "grey")
)
}
myGlist <- within(myGlist, rm(target_name))
gridExtra::grid.arrange(grobs = myGlist, ncol = 3)
}
plot_corr_matrix <- function(dataframe, significance_threshold){
title <- paste0('Correlation Matrix for significance > ',
significance_threshold)
df_cor <- dataframe %>% mutate_if(is.character, as.factor)
df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
#run a correlation and drop the insignificant ones
corr <- cor(df_cor)
#prepare to drop duplicates and correlations of 1
corr[lower.tri(corr,diag=TRUE)] <- NA
#drop perfect correlations
corr[corr == 1] <- NA
#turn into a 3-column table
corr <- as.data.frame(as.table(corr))
#remove the NA values from above
corr <- na.omit(corr)
#select significant values
corr <- subset(corr, abs(Freq) > significance_threshold)
#sort by highest correlation
corr <- corr[order(-abs(corr$Freq)),]
#print table
# print(corr)
#turn corr back into matrix in order to plot with corrplot
mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
#plot correlations visually
corrplot(mtx_corr,
title=title,
mar=c(0,0,1,0),
method='color',
tl.col="black",
na.label= " ",
addCoef.col = 'black',
number.cex = .9)
}
test_URL <- paste0('https://raw.githubusercontent.com/AngelClaudio/',
'data-sources/master/csv/insurance-evaluation-data.csv')
train_URL <- paste0('https://raw.githubusercontent.com/AngelClaudio/',
'data-sources/master/csv/insurance_training_data.csv')
train <- readr::read_csv(train_URL)
test <- readr::read_csv(test_URL)
download.file(url = paste0('https://raw.githubusercontent.com/AngelClaudio/',
'data-sources/master/Picts/insurance%20logos%206.png'),
destfile = "image.png",
mode = 'wb')
knitr::include_graphics(path = "image.png")
glimpse(train)
dim(train %>% select_if(is.numeric))[2]
dim(train %>% select_if(is.character))[2]
summary(train)
colSums(is.na(train))
VIM::aggr(train, col=c('green','red'), numbers=T, sortVars=T,
cex.axis = .7,
ylab=c("Proportion of Data", "Combinations and Percentiles"))
# ***We don't need to show or talk about this, this is for our sake, audience
#    doesn't care.
# bind data to make transformations easier
train$dataset <- 'train'
test$dataset <- 'test'
final_df <- rbind(train, test)
final_df <-  dplyr::select(final_df, -c('INDEX'))
glimpse(dplyr::select(final_df, -c('dataset')))
final_df  <- dplyr::mutate(final_df, INCOME = as.numeric(gsub('[$,]', '', INCOME)),
HOME_VAL = as.numeric(gsub('[$,]', '', HOME_VAL)),
BLUEBOOK = as.numeric(gsub('[$,]', '', BLUEBOOK)),
OLDCLAIM = as.numeric(gsub('[$,]', '', OLDCLAIM)))
glimpse(dplyr::select(final_df, c('INCOME','HOME_VAL', 'BLUEBOOK','OLDCLAIM')))
final_df <- dplyr::mutate(final_df,
MSTATUS = as.factor(str_remove(MSTATUS, "^z_")),
SEX = as.factor(str_remove(SEX, "^z_")),
# <HIgh School not a typo, means less than HS
EDUCATION = as.factor(str_remove(EDUCATION, "^z_")),
JOB = as.factor(str_remove(JOB, "^z_")),
CAR_TYPE = as.factor(str_remove(CAR_TYPE, "^z_")),
URBANICITY = as.factor(str_remove(URBANICITY, "^z_")),
CAR_USE = as.factor(CAR_USE),
REVOKED = as.factor(REVOKED),
PARENT1 = as.factor(PARENT1),
RED_CAR = as.factor(RED_CAR),
TARGET_FLAG = as.factor(TARGET_FLAG))
sapply(dplyr::select(final_df, c('MSTATUS','SEX', 'JOB', 'CAR_TYPE',
'URBANICITY', 'CAR_USE', 'REVOKED','PARENT1',
'RED_CAR', 'TARGET_FLAG')), levels)
# impute numerics
final_df <- final_df %>% mutate_at(vars(c("CAR_AGE", "YOJ", "AGE", "INCOME",
"HOME_VAL")),
~ifelse(is.na(.), na.approx(.), .)) #na.approx from the zoo library to perform linear interpolation on NA values
# impute NAs in job
final_df$JOB <- as.character(final_df$JOB)
final_df$JOB[is.na(final_df$JOB)] <- "Unknown"
final_df$JOB <- as.factor(final_df$JOB)
sapply(dplyr::select(final_df, c('JOB')), levels)
sapply(final_df, function(x) sum(is.na(x)))
VIM::aggr(final_df, col=c('green','red'), numbers=T, sortVars=T,
cex.axis = .7,
ylab=c("Proportion of Data", "Combinations and Percentiles"))
glimpse(dplyr::select(final_df, -c('dataset')))
# unbind data
train <- dplyr::select(dplyr::filter(final_df, dataset == 'train'),
-c('dataset'))
test <- dplyr::select(dplyr::filter(final_df, dataset == 'test'),
-c('dataset'))
inspectdf::inspect_imb(train) %>% show_plot()
inspectdf::inspect_num(train) %>% show_plot()
# target_name <- 'your_target_name'
target_name <- 'TARGET_FLAG'
boxplot_depend_vs_independ(train, target_name)
plot_corr_matrix(train, .2)
# flag brand new cars
final_df$CAR_AGE <- ifelse(final_df$CAR_AGE < 1, 1, final_df$CAR_AGE)
final_df$CAR_AGE_BRAND_NEW_FLAG <- ifelse(final_df$CAR_AGE == 1, 1, 0)
# Claim Frequency
# table(train$CLM_FREQ)
# zero claims
final_df$CLM_FREQ_ZERO <- ifelse(final_df$CLM_FREQ == 0, 1, 0)
# Home Value
# table(train$HOME_VAL)
# no homes? renters?
final_df$HOME_VAL_ZERO <- ifelse(final_df$HOME_VAL == 0, 1, 0)
# Motor Vehicle Record Points
# table(train$MVR_PTS)
# no motor vehicle record points, clean record
final_df$MVR_PTS_ZERO <- ifelse(final_df$MVR_PTS == 0, 1, 0)
# no years on job
# table(train$YOJ)
final_df$YOJ_ZERO <- ifelse(final_df$YOJ == 0, 1, 0)
glimpse( final_df )
train_data <- final_df %>% filter(dataset == 'train') %>% dplyr::select(-dataset)
# remove target amt since it has a whole bunch of NAs
train_data$TARGET_AMT <- NULL
split <- caret::createDataPartition(train_data$TARGET_FLAG, p=0.85, list=FALSE)
partial_train <- train_data[split, ]
validation <- train_data[ -split, ]
#I changed the order to move from conceptually simple models to more complex (randomforest)
binary.mdl <- glm(TARGET_FLAG~., family=binomial, data=partial_train)
summary(binary.mdl)
binary.mdl.w.step <- step(binary.mdl)
summary(binary.mdl.w.step)
rf <- randomForest::randomForest(TARGET_FLAG~., data=partial_train)
rf
train_mv_mdl <- final_df %>% filter(dataset == 'train') %>% dplyr::select(-dataset)
train_mv_mdl <- train_mv_mdl[train_mv_mdl$TARGET_FLAG == 1, ]
train_mv_mdl$TARGET_FLAG <- NULL
split_mv <- caret::createDataPartition(train_mv_mdl$TARGET_AMT, p=0.85, list = F)
partial_train_mv <- train_mv_mdl[split_mv, ]
validation_mv <- train_mv_mdl[-split_mv, ]
#summary(partial_train_mv)
mv.mdl <- train(TARGET_AMT ~., data = partial_train_mv, method = "lm",
trControl = trainControl(method = "cv", number = 10,
savePredictions = TRUE),
tuneLength = 5, preProcess = c("center", "scale"))
summary(mv.mdl$finalModel)
lm1 <- mv.mdl
lm2_base <- lm(TARGET_AMT ~ ., data = partial_train_mv)
lm_step <- stepAIC(lm2_base, trace = F)
summary(lm_step)
lm2 <- lm_step
train_mv_mdl %>%
select_if(is.numeric) %>%
dplyr::mutate(row = row_number()) %>%
tidyr::gather(field, val, -row) %>%
dplyr::filter(val > 0) %>%
tidyr::spread(field, val) %>%
dplyr::select(-row, -CAR_AGE_BRAND_NEW_FLAG, -CLM_FREQ_ZERO, -HOME_VAL_ZERO, -MVR_PTS_ZERO, -YOJ_ZERO) %>%
powerTransform()
lm3_base <- lm(log(TARGET_AMT) ~ . + I(BLUEBOOK^0.5) + I(MVR_PTS^.33) + I(CAR_AGE^.5) + I(CLM_FREQ^0.33)
, data = partial_train_mv)
summary(lm3_base)
lm3_step <- stepAIC(lm3_base, trace = F, direction = 'backward')
summary(lm3_step)
lm3 <- lm3_step
# plot(lm3)
# partial_train_mv$pred <- predict(lm3)
#
# ggplot(partial_train_mv, aes(x = pred, y = log(TARGET_AMT))) +
#   geom_point(alpha = .2) +
# geom_smooth(se = F, method = 'lm')
lm4_base <- lm(TARGET_AMT ~ ., data = partial_train_mv)
resid_sq <- lm4_base$residuals^2
lm4_wls <- lm(TARGET_AMT ~ ., data = partial_train_mv, weights = 1/resid_sq)
#summary(lm4_wls)
lm4_wls_step <- stepAIC(lm4_wls, trace = F)
summary(lm4_wls_step)
# partial_train_mv$pred <- predict(lm4_wls_step)
#
# ggplot(partial_train_mv, aes(x = pred, y = TARGET_AMT)) +
#   geom_point(alpha = .2) +
#   geom_smooth(se = F, method =)
# plot(lm4_wls_step)
lm4 <- lm4_wls_step
y_hat_glm <- predict(binary.mdl, validation, type = "response")
y_hat_glm_binar <- (y_hat_glm>0.5)*1
mean(validation$TARGET_FLAG==y_hat_glm_binar)
(CM <- table(true= validation$TARGET_FLAG, predicted = y_hat_glm_binar))
y_hat_glm <- predict(binary.mdl.w.step, validation, type = "response")
y_hat_glm_binar <- (y_hat_glm>0.5)*1
mean(validation$TARGET_FLAG==y_hat_glm_binar)
(CM <- table(true= validation$TARGET_FLAG, predicted = y_hat_glm_binar))
val <- validation
val$pred <- predict(rf, val)
val$pred <- as.factor(val$pred)
random_forest_results <- confusionMatrix(val$pred, val$TARGET_FLAG)
random_forest_results
validation_mv$model_4 <- predict(lm1, newdata = validation_mv)
validation_mv$model_5 <- predict(lm2, newdata = validation_mv)
validation_mv$model_6 <- predict(lm3, newdata = validation_mv)
validation_mv$model_7 <- predict(lm4, newdata = validation_mv)
validation_mv %>%
dplyr::select(TARGET_AMT, model_4, model_5, model_6, model_7) %>%
tidyr::gather(model, prediction, -TARGET_AMT) %>%
ggplot(aes(x = TARGET_AMT, y = prediction)) +
geom_point(alpha = .2) +
geom_abline(slope = 1, intercept = 0, alpha = .3) +
facet_wrap(~model)
#To further support this, we examine the model residuals as a function target variable. The residual variance is not constant as a function of `TARGET_AMT` which suggest that there is room for improvement for accurately predicting the numeric target variable (the amount of money it will cost if a person crashes their car).
# validation_mv$Model_4 <- with(validation_mv, pred1 - TARGET_AMT)
# validation_mv$Model_5 <- with(validation_mv, pred2 - TARGET_AMT)
# validation_mv$Model_6 <- with(validation_mv, pred3 - TARGET_AMT)
# validation_mv$Model_7 <- with(validation_mv, pred4 - TARGET_AMT)
#
# validation_mv %>%
#   dplyr::select(TARGET_AMT, Model_4, Model_5, Model_6, Model_7) %>%
#   tidyr::gather(model, residuals, -TARGET_AMT) %>%
#   ggplot(aes(x = TARGET_AMT, y = residuals)) +
#   geom_point(alpha = .2) +
#   geom_hline(yintercept = 0, alpha = .3) +
#   facet_wrap(~model)
# cat('Model 4:', cor(validation_mv$TARGET_AMT, validation_mv$model_4)^2)
# cat('Model 5:', cor(validation_mv$TARGET_AMT, validation_mv$model_5)^2)
# cat('Model 6:', cor(validation_mv$TARGET_AMT, validation_mv$model_6)^2)
# cat('Model 7:', cor(validation_mv$TARGET_AMT, validation_mv$model_7)^2)
validation_mv$resid_4 <- with(validation_mv, model_4 - TARGET_AMT)
validation_mv$resid_5 <- with(validation_mv, model_5 - TARGET_AMT)
validation_mv$resid_6 <- with(validation_mv, model_6 - TARGET_AMT)
validation_mv$resid_7 <- with(validation_mv, model_7 - TARGET_AMT)
cat('RMSE\n',
'Model 4: ', sqrt(sum(validation_mv$resid_4^2) / nrow(validation_mv)), '\n',
'Model 5: ', sqrt(sum(validation_mv$resid_5^2) / nrow(validation_mv)), '\n',
'Model 6: ', sqrt(sum(validation_mv$resid_6^2) / nrow(validation_mv)), '\n',
'Model 7: ', sqrt(sum(validation_mv$resid_7^2) / nrow(validation_mv))
)
test <- final_df %>% filter(dataset == 'test') %>% dplyr::select(-dataset)
logistic_binary_final <- predict(binary.mdl.w.step, test, type = "response")
head(logistic_binary_final)
MVPred <- predict(lm2, newdata = test)
head( MVPred)
